\chapter{Introduction}\label{chapter:motivation}\thispagestyle{scrheadings}

With the rise of cloud computing, the load in modern data centers is less and less uniform. %During peak hours, the cloud tenants use significant parts of the available infrastructure. The tenants themselves can scale the applications based on user demand. Cloud vendors have adopted dynamic, demand-based scaling by providing not only for renting out \acp{VM} but also a vast set of higher-level services, such as key-value stores, persistent object storage, or relational databases.
Cloud providers sell not only \acp{VM} and raw storage, so-called \ac{IaaS}, but also hosted services, like databases or firewalls, as \ac{PaaS}. The cloud vendor manages the services and guarantees their availability. The vendors bill the usage of \ac{PaaS} products in a fine-granular manner, for example, per database transactions. This cloud model gained popularity in the last decade, as cloud customers do not have to administrate their own \acp{VM} and storage, which they instead rent from cloud vendors as \ac{IaaS}. The advantage for the customers is the demand-based pricing and externalization of the administrative work of running the services themselves, resulting in lower management overhead for the customers. Further, the centralization of services allows for higher utilization.

The next leap in cloud infrastructure development is \ac{FaaS}, also known as serverless computing. In serverless computing, customers define their applications as pipelines of functions that connect different cloud services. Serverless applications are more flexible and scalable because all custom application parts can now run on the cloud provider's scalable and distributed runtime instead of a customer-managed \ac{VM}. For example, consider an application where a user's web request or the creation event of a storage object can trigger a serverless function, which loads storage objects, performs data-analysis tasks, and returns a result to the user. The cloud provides a web request service that triggers the function, which dynamically starts in an isolated environment. All prominent cloud vendors have \ac{FaaS} products, which became extremely popular in the last years~\cite{eskandaniUphillJourneyFaaS2023, datadogStateServerless, schleier-smithWhatServerlessComputing2021}\@.
\ac{AWS} Lambda\footnote{\url{https://aws.amazon.com/lambda/}}, Azure Functions\footnote{\url{https://learn.microsoft.com/en-us/azure/azure-functions/functions-overview}}, and Google Cloud Functions\footnote{\url{https://cloud.google.com/functions/}} are the serverless product lines by the largest three cloud vendors.
%These \ac{FaaS} functions are traditionally state-less, but large cloud providers have started providing state-full \ac{FaaS} environments~\cite{cgillumDurableFunctionsOverview2023, jameswardStatefulServerlessGoogle}\@.

This serverless design also benefits cloud application developers because they can connect a vast library of standard functions and connect \ac{PaaS} components with little or no coding required.
A tenant only needs to develop the composition of predefined functions, is not required to know their inner workings, and does not have to bother with the optimizations for the specific hardware platform. Applications for this infrastructure can often be developed in a low- or no-code environment to minimize cloud customers' work. The cloud providers entirely handle the exact implementations. The benefit becomes even more significant for tenants, who can easily integrate complex \ac{ML} applications as part of the processing pipeline. For example, an image classifier can be composed with the rest of the application without dealing with inference hardware and software support setup. The advantage for the cloud provider is the possibility of sharing the accelerator and loaded model between different tenants to maximize the utilization of the resources by exposing a simple state-less \ac{API} to the \ac{FaaS} applications.

\ac{ML} scenarios and other compute-heavy workloads require specialized accelerators, which have become ubiquitous in modern cloud data centers. The disaggregation of the cloud infrastructure allows the provider to compose the host configurations depending on the customer demand. Different types of computational resources, like CPUs, Storage, and Accelerators (\acp{GPU}, \ac{FPGA}, \ldots), are separated into different servers, interconnected via low-latency networks. Thus, the cloud provider can construct a custom host configuration based on the customer's wishes. The disaggregation of data centers allows for higher utilization of resources \cite{duatoRCUDAReducingNumber2010, intelIntelRackScale2014, linDisaggregatedDataCenters2020a}, but requires special interconnects, like infiband or \ac{CXL}~\cite{HomepageComputeExpress2023}, to provide low latency networks. Besides the scalability benefits, disaggregation of compute resources in data centers introduces additional runtime overheads for the authentication mechanism and distributed resource management. Disaggregated architectures are rarely seen in public cloud infrastructures, as these mostly depend on ethernet networks, which suffer from high and high-variance network latencies~\cite{popescuMeasuringNetworkConditions2021}.\@

Using shared resources requires a robust isolation mechanism to prevent cross-tenant data access, even in the case of a security vulnerability in the shared service. The isolation between the functions of different cloud tenants within the provider's \ac{FaaS} runtime can either be through language isolation, containerization, or in the case of \ac{AWS} Lambda lightweight \acp{VM} using Amazon Firecracker~\cite{agacheFirecrackerLightweightVirtualization2020}.\@ Besides isolating customer functions, the runtime must prevent access to \ac{PaaS} services if the application does not have the required permissions. Commonly, \ac{FaaS} runtimes use static access tokens for fine-grained permissions and \acp{ACL} for isolating applications from accessing services entirely. Static authentication tokens are problematic because the tokens often provide full access to a service and are rarely changed.
Using two parallel, static isolation schemes yields complex permission management, which customers often misconfigure~\cite{huangConfValleySystematicConfiguration2015}.\@
Previous research in distributed execution environments, like \ac{FaaS} runtimes, proposes capability-based authentication mechanisms~\cite{tanenbaumDistributedOperatingSystems1985, vilanovaSlashingDisaggregationTax2022}.\@
This work uses a single distributed capability system as a simpler access model, which allows lightweight isolation through the capabilities and the programming language.
% - Why capabilities
% Existing \ac{FaaS} runtimes use global access tokens or \acp{ACL} as authentication mechanisms. The usage of \acp{ACL} requires a constant reconfiguration of the enforcement layer, depending on where the runtime schedules the execution of the function.

The FractOS meta operating system~\cite{vilanovaSlashingDisaggregationTax2022} achieves all the goals mentioned above as it provides a capability-based distributed execution environment tailored for disaggregation by using low-latency \ac{RDMA} networking. Thus, this work builds on the FractOS design and aims to eliminate one of its weaknesses.
FractOS tries to offload the resource management onto SmartNICs and remove all mangement tasks from the host \ac{CPU}. The evaluation of FractOS shows that this offloading introduces a significant latency degradation. This work tries to remove some of the capability systems complexity by centralizing it into programmable switches.
Chapter~\ref{chapter:design} discusses the design proposal for an in-network capability system leveraging novel programmable networking hardware and presents a proof-of-concept \ac{FaaS} runtime and user library using this capability system. The evaluation in Chapter~\ref{chapter:evaluation} shows that, besides the underlying network latencies, the performance is comparable to FractOS.\@ The in-network acceleration effectively removes load from the compute nodes.
% implementation's latency overheads are caused by  shows no performance degradation in regard to \ac{RPC} latency compared to FractOS~\cite{vilanovaSlashingDisaggregationTax2022}\@.
%With the disaggregated usage scenario in mind, this work aims to accelerate the permission enforcement using programmable network switches. The basis of this work is the FractOS meta operating system, which is the inspiration for the distributed programming model and capability based authentication mechanism this work uses.
% - CXL as possible interconnect
%
% - nvida proprietery shit for GPUs
%
% - RPC based architectures with low CPU high GPU hosts
%
% \begin{itemize}
%   \item Motivation for disaggregation
%   \item Problems in current disaggregated architectures
%   \item Offloading systems task as a common method for better resource utilization
%
% \end{itemize}

% \color{red}{TODO: seperation in introduction and motivation}\color{black}
